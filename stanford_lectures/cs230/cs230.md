# CS230 Notes


## lecture 2.

- a model is defined as the architecture & the parameters
- there are a bunch of things that we can change for the model
    - input
    - output
    - architecture
        - activation function
        - optimizers
        - hyper parameters
    - loss function — there are many loss functions to choose from *i.e* MSE, cross entropy, etc
- **understanding logistic regression**
    - we have an input matrix which we flatten to get a vector
        - we have a vector which we forward propagate then we multiply with weight *w* and add bias *b*
            - then we pass this to a sigmoid (activation) function and get an output
    - if we want to multi-class logistic regression, then we have multiple neurons in the last layer
    - understanding labelling
        - one-hot encoding is a way to go about labelling — all 0s and a singular 1 for that label
            - downside is that you can’t identify images with more than 1 label *i.e* an image with a dog AND a cat — work around is using **multi-hot encoding**
                - works very well when you’re guaranteed to have 1 class per input
- how do you figure out how much data you need?
    - depending on the complexity of the task, we determine how much data we need
        - for example, if we want to build a simple classifier that determines if an image if day or night, that’s a very simple task, we don’t need that much data (~ 10000 images if only outside pictured, 100k if we want to train it on outside + inside images)
- when it comes to images, aim to have lower resolution images
- day and night model
    - the last activation function we should use is sigmoid since it takes all numbers between -inf to inf and outputs it as a number between 0 and 1
    - shallow network would work well here
    - we can use logistic loss (negative log) — we choose this one because it’s easier to optimize than other loss functions
- face ID verification
    - resolution of image is (412x412) — need higher resolution because some people may look similar *i.e* twins
    - we are going to encoder the information about the picture in a vector (encoding generates a vector)
    - we need to generate triplets
        - anchor, positive, negative
            - we want to minimize the distance between anchor and positive (they are the same people, but different images)
            - we want to maximize the distance between anchor and negative (different people)
        - the loss function would be the distance between encoding of anchor and encoding of positive subtracted by the distance between encoding of anchor and encoding of negative
            - we add a small value of alpha to the loss because sometimes the distance may return 0, but we still want the network to learn something
- face recognition
    - we can train a KNN for this
        - we can use elbow method (learn this) to figure out the **k**
    - to figure out how many layers we need to go for encoding for something like face recognition depends on complexity
        - for example, for face recognition, we want to figure out things like distance between eyes and nose — we would first get the first layer to detect edges, then 2nd layer to detect eyes and nose, 3rd layer to detect the distance, etc.
- art generation → making an image “beautiful”
    - our data would be the style of image we want to generate — for example a painting could be a style
    - we want to use a model that understands images very well — we load an existing model (imagenet for example)
        - when this image forward propagates, we can get information about its content and its style by inspecting the layers
        - we then take the style image and use the Gram matrix to extract the style of the style image
    - loss function will be ||style_s - style_c|| + ||content_c - content_g||
        - style_s is the style of the style image, style_c is the style of the content image, content_c is the content of the content image and content_g is the content of the generated image
    - we are not training the network to learn parameters by minimizing the loss, but rather we are learning an image
- trigger word detection — detect when someone says the word “activate”
    - we would encode the sound data with 0s and 1s per sample — 1 would only appear when (more specifically right after it is said) the trigger word is said
    - architecture should be a RNN
    - negative log loss function — we would compute the loss at every time step
    - data labelling
        - get 3 databases: positive words, negative words, background noise
            - then we generate data where we have 10s of background noise, randomly insert negative words and then insert a positive word (ensure no overlap)

## lecture 3.

- in order to build a DL project / application, there a couple of steps involved
    1. find problem
    2. collect data
    3. decide on model architecture
    4. train model
    5. test model
    6. deploy

## lecture 4-6.

- these lectures covered GANs which i’ve already went over + 2 other lectures which were around AI & healthcare and something specific to the course project
    - imo, not the most valuable lectures, decided to skip over them since i already know most of the content

## lecture 7 — interpretability of neural networks.

- let’s say we have a model that classifies animals — how can we prove that the model is actually looking at a cat when we show the model a picture of it?
    - if we have a CNN that’s trained with a softmax output layer → outputs a probability distribution
        - we take the scores pre-softmax, we want to take the derivative of the score of the cat and backpropagate it to the input — this basically will tell us which parts of the inputs were discriminative for this score
            - when we do this, it tells us which pixels need to be changed the least to impact the class score the most
                - more intuitively, the backpropagated matrix is the same size as the input — the numbers with high absolute value, the pixels corresponding to those locations had an impact on the score of cat
        - another idea is to take the scores before the softmax and minimize all the scores of the classes that aren’t “cat”
- **saliency maps**
    - with the pixels that have a stronger influence on the score, we can use that to segment the image — this can be done using a simple thresholding value
        - saliency maps is a quick technique to visualize what the network is looking at
- **occlusion sensitivity**
    - let’s say we’re trying to classify the same dog / cat image from before, but we cover some pixels up with a grey square — now we test how much impact the square has in different locations
        - where ever the output is the lowest, that’s where the dog most likely is / where the most important pixels are
        - **important:** it may also be that the square improves the probability, for example if there’s a human and a dog in the picture, if the square covers up the human, the model will most likely be able to perform better
- **class activation maps**
    - classification networks have really good localization ability
        - off topic, but your typical CNN structure looks like this
            - a bunch of conv, relu, max pool layers (in that exact order) and then you flatten the output, pass it through a couple FCC layers, apply softmax and then you get the output. the FCC players the role of the classifier
    - building off of that idea, to produce a class activation map, we get rid of the flatten layer as it gets rid of all the spatial information as everything is flattened into one vector
        - instead we use a global average pooling layer — we take all the feature maps produced by the previous conv layer and then take the average of each feature map
            - for example if the dimension was (4,4,6) — this corresponds to 6 feature maps with dimension 4x4 — then the new output after global avg pooling would be (1,1,6)
                - after this, we pass it through a single FCC + softmax and it outputs a bunch of probabilities corresponding to each class
    - the feature maps actually contain some **visual patterns**
        - and there’ll be some parts that are lit up and that tells you the activations have found something in those spots — you can repeat this process for all the feature maps
            - this basically means there was a visual pattern in the input that activated the feature map
                
                ![Screenshot 2024-07-13 at 7.26.19 PM.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/c9aa599c-2115-4330-846e-652102e8621e/5fdf3b37-7592-4cef-b54f-dceec38518f1/Screenshot_2024-07-13_at_7.26.19_PM.png)
                
        - looking at the image above, the score of dog is 91% — now we can reverse engineer and see how much of that score came from each of these feature maps
            - if we take a weighted average and sum it all up, you’ll get another feature map which is the **class activation map** for “dog”
                
                ![Screenshot 2024-07-13 at 7.29.02 PM.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/c9aa599c-2115-4330-846e-652102e8621e/fb9f6ea2-190a-4d75-aa27-c1a67c393784/Screenshot_2024-07-13_at_7.29.02_PM.png)
                
            - you can see that it was probably highly influence by the 2nd feature map as that’s components of the dog
    - **dataset search**
        - take a feature map from the last conv layer and then find examples in the dataset which coordinate to that feature map — you will likely find a common trend and you’ll know what that feature map was looking for
        -