# CS230 Notes


## lecture 2.

- a model is defined as the architecture & the parameters
- there are a bunch of things that we can change for the model
    - input
    - output
    - architecture
        - activation function
        - optimizers
        - hyper parameters
    - loss function — there are many loss functions to choose from *i.e* MSE, cross entropy, etc
- **understanding logistic regression**
    - we have an input matrix which we flatten to get a vector
        - we have a vector which we forward propagate then we multiply with weight *w* and add bias *b*
            - then we pass this to a sigmoid (activation) function and get an output
    - if we want to multi-class logistic regression, then we have multiple neurons in the last layer
    - understanding labelling
        - one-hot encoding is a way to go about labelling — all 0s and a singular 1 for that label
            - downside is that you can’t identify images with more than 1 label *i.e* an image with a dog AND a cat — work around is using **multi-hot encoding**
                - works very well when you’re guaranteed to have 1 class per input
- how do you figure out how much data you need?
    - depending on the complexity of the task, we determine how much data we need
        - for example, if we want to build a simple classifier that determines if an image if day or night, that’s a very simple task, we don’t need that much data (~ 10000 images if only outside pictured, 100k if we want to train it on outside + inside images)
- when it comes to images, aim to have lower resolution images
- day and night model
    - the last activation function we should use is sigmoid since it takes all numbers between -inf to inf and outputs it as a number between 0 and 1
    - shallow network would work well here
    - we can use logistic loss (negative log) — we choose this one because it’s easier to optimize than other loss functions
- face ID verification
    - resolution of image is (412x412) — need higher resolution because some people may look similar *i.e* twins
    - we are going to encoder the information about the picture in a vector (encoding generates a vector)
    - we need to generate triplets
        - anchor, positive, negative
            - we want to minimize the distance between anchor and positive (they are the same people, but different images)
            - we want to maximize the distance between anchor and negative (different people)
        - the loss function would be the distance between encoding of anchor and encoding of positive subtracted by the distance between encoding of anchor and encoding of negative
            - we add a small value of alpha to the loss because sometimes the distance may return 0, but we still want the network to learn something
- face recognition
    - we can train a KNN for this
        - we can use elbow method (learn this) to figure out the **k**
    - to figure out how many layers we need to go for encoding for something like face recognition depends on complexity
        - for example, for face recognition, we want to figure out things like distance between eyes and nose — we would first get the first layer to detect edges, then 2nd layer to detect eyes and nose, 3rd layer to detect the distance, etc.
- art generation → making an image “beautiful”
    - our data would be the style of image we want to generate — for example a painting could be a style
    - we want to use a model that understands images very well — we load an existing model (imagenet for example)
        - when this image forward propagates, we can get information about its content and its style by inspecting the layers
        - we then take the style image and use the Gram matrix to extract the style of the style image
    - loss function will be ||style_s - style_c|| + ||content_c - content_g||
        - style_s is the style of the style image, style_c is the style of the content image, content_c is the content of the content image and content_g is the content of the generated image
    - we are not training the network to learn parameters by minimizing the loss, but rather we are learning an image
- trigger word detection — detect when someone says the word “activate”
    - we would encode the sound data with 0s and 1s per sample — 1 would only appear when (more specifically right after it is said) the trigger word is said
    - architecture should be a RNN
    - negative log loss function — we would compute the loss at every time step
    - data labelling
        - get 3 databases: positive words, negative words, background noise
            - then we generate data where we have 10s of background noise, randomly insert negative words and then insert a positive word (ensure no overlap)

## lecture 3.

- in order to build a DL project / application, there a couple of steps involved
    1. find problem
    2. collect data
    3. decide on model architecture
    4. train model
    5. test model
    6. deploy

## lecture 4-6.

- these lectures covered GANs which i’ve already went over + 2 other lectures which were around AI & healthcare and something specific to the course project
    - imo, not the most valuable lectures, decided to skip over them since i already know most of the content

## lecture 7 — interpretability of neural networks.

- let’s say we have a model that classifies animals — how can we prove that the model is actually looking at a cat when we show the model a picture of it?
    - if we have a CNN that’s trained with a softmax output layer → outputs a probability distribution
        - we take the scores pre-softmax, we want to take the derivative of the score of the cat and backpropagate it to the input — this basically will tell us which parts of the inputs were discriminative for this score
            - when we do this, it tells us which pixels need to be changed the least to impact the class score the most
                - more intuitively, the backpropagated matrix is the same size as the input — the numbers with high absolute value, the pixels corresponding to those locations had an impact on the score of cat
        - another idea is to take the scores before the softmax and minimize all the scores of the classes that aren’t “cat”
- **saliency maps**
    - with the pixels that have a stronger influence on the score, we can use that to segment the image — this can be done using a simple thresholding value
        - saliency maps is a quick technique to visualize what the network is looking at
        -