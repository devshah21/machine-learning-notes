# CS224N - Natural Language Processing w/ Deep Learning

## lecture 1.

- the problem with traditional NLP is that we regard words as discrete symbols
    - hotel, conference, motel — this is a localist representation
- we can use distributional semantics to figure out a word’s meaning → the idea is that we can give a word meaning by the words that appear around it frequently
- **word vectors**
    - each word is assigned a dense vector and words that are similar to that word will have similar word vectors
        - for example, cat and dog will have similar word vectors since they are very similar
        - word vectors are also called word embeddings. they are a distributed representation (since the word meaning is spread across many values in the vector)
- **word2vec**
    - this is a framework to learn word vectors; it was proposed in 2013
    - how it works:
        - we have a large body of text and we assign each word a vector
        - we go through every position in the text (*t),* which has a center word (*c*) and context words (*o*)
        - using the similarity of the word vectors for c and o to compute probability of o given c (and we do this the other way around as well)
        - we keep adjusting word vectors to maximize the probability
            
            ![Screenshot 2024-07-28 at 9.02.36 PM.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/c9aa599c-2115-4330-846e-652102e8621e/4f011f79-b8cb-4eab-abba-59f49b78ce53/Screenshot_2024-07-28_at_9.02.36_PM.png)
            
    - for each position t = 1, 2, 3, … T, we predict the context words within a fixed size window (*m*), given a center word w_j
    - $L(\theta) = \prod_{t=1}^T \prod_{\substack{-m \leq j \leq m \\ j \neq 0}} P(w_{t+j} \mid w_t; \theta)$
        - the outer product runs from t=1 to T, covering all words in the body of text. for each word w_t, at position t, we consider a context window around it
        - the inner product sums over all positions j in the context window
            - j ranges from -m to m, so it considers both previous and next words within the context window
        - theta represents the parameters that need to be optimized
        - T is the total number of words in the body of text
        - m is the size of the context video
        - the probability function is for the context word w_t+j given a target word w_t. it basically represents the probability of observing the word w_t+j in the context of the word w_t
    - the objective function J(theta) is the average negative log likelihood
        
        ![Screenshot 2024-07-28 at 9.36.53 PM.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/c9aa599c-2115-4330-846e-652102e8621e/ac2f6f7c-1f3c-4451-a9d4-6504ebc6f57d/Screenshot_2024-07-28_at_9.36.53_PM.png)
        
        - we aim to minimize this function and in return, it helps maximize our predictive accuracy
    - how to compute the probability function?
        - we use 2 vectors per word
            - first vector is when it’s a center word
            - second vector is when it’s a context word
    - prediction function
        
        ![Screenshot 2024-07-28 at 9.39.58 PM.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/c9aa599c-2115-4330-846e-652102e8621e/fae4a828-e7c8-40f7-ab11-0d377fc821ed/Screenshot_2024-07-28_at_9.39.58_PM.png)
        
        - we take the dot product because it helps us compute and understand the similarity of o and c
            - from linear algebra, larger dot product ⇒ larger probability
        -