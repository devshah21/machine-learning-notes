# CS224N - Natural Language Processing w/ Deep Learning

## lecture 1.

- the problem with traditional NLP is that we regard words as discrete symbols
    - hotel, conference, motel — this is a localist representation
- we can use distributional semantics to figure out a word’s meaning → the idea is that we can give a word meaning by the words that appear around it frequently
- **word vectors**
    - each word is assigned a dense vector and words that are similar to that word will have similar word vectors
        - for example, cat and dog will have similar word vectors since they are very similar
        - word vectors are also called word embeddings. they are a distributed representation (since the word meaning is spread across many values in the vector)
- **word2vec**
    - this is a framework to learn word vectors; it was proposed in 2013
    - how it works:
        - we have a large body of text and we assign each word a vector
        - we go through every position in the text (*t),* which has a center word (*c*) and context words (*o*)
        - using the similarity of the word vectors for c and o to compute probability of o given c (and we do this the other way around as well)
        - we keep adjusting word vectors to maximize the probability
            
            ![Screenshot 2024-07-28 at 9.02.36 PM.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/c9aa599c-2115-4330-846e-652102e8621e/4f011f79-b8cb-4eab-abba-59f49b78ce53/Screenshot_2024-07-28_at_9.02.36_PM.png)
            
    - for each position t = 1, 2, 3, … T, we predict the context words within a fixed size window (*m*), given a center word w_j
    - $L(\theta) = \prod_{t=1}^T \prod_{\substack{-m \leq j \leq m \\ j \neq 0}} P(w_{t+j} \mid w_t; \theta)$
        - the outer product runs from t=1 to T, covering all words in the body of text. for each word w_t, at position t, we consider a context window around it
        - the inner product sums over all positions j in the context window
            - j ranges from -m to m, so it considers both previous and next words within the context window
        - theta represents the parameters that need to be optimized
        - T is the total number of words in the body of text
        - m is the size of the context video
        - the probability function is for the context word w_t+j given a target word w_t. it basically represents the probability of observing the word w_t+j in the context of the word w_t
    - the objective function J(theta) is the average negative log likelihood
        
        ![Screenshot 2024-07-28 at 9.36.53 PM.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/c9aa599c-2115-4330-846e-652102e8621e/ac2f6f7c-1f3c-4451-a9d4-6504ebc6f57d/Screenshot_2024-07-28_at_9.36.53_PM.png)
        
        - we aim to minimize this function and in return, it helps maximize our predictive accuracy
    - how to compute the probability function?
        - we use 2 vectors per word
            - first vector is when it’s a center word
            - second vector is when it’s a context word
    - prediction function
        
        ![Screenshot 2024-07-28 at 9.39.58 PM.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/c9aa599c-2115-4330-846e-652102e8621e/fae4a828-e7c8-40f7-ab11-0d377fc821ed/Screenshot_2024-07-28_at_9.39.58_PM.png)
        
        - we take the dot product because it helps us compute and understand the similarity of o and c
            - from linear algebra, larger dot product ⇒ larger probability
        - softmax is applied here as well — we use exponential to make everything positive and we divide by the overall sum to get all the probabilities between 0 and 1
    - **how to minimize loss? (**model training**)**
        - to train the model, we are gradually adjusting the parameters to minimize the loss
            - remember that theta is a large  vector with all the model parameters
            - in our case, we have *d* dimensioanl vectors and *V* many words
                - recall that every word has 2 vectors (when it’s a center word and when it’s a context word)
                    
                    ![Screenshot 2024-08-01 at 5.53.47 PM.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/c9aa599c-2115-4330-846e-652102e8621e/40de4c60-feb7-4e52-9eda-3fdfcb8abbaa/Screenshot_2024-08-01_at_5.53.47_PM.png)
                    
            - in the image above, we optimize all the parameters by walking down the gradient (minimizing the loss by computing all the vector gradients)

## lecture 2.

- **bag of words model (word2vec)**
    - this model makes the same predictions at each position
        - to develop the intuition here, the models have gotten better over time. we want a model that gives a reasonably high probability estimate to all words that occur in the context
    - parameters
        - the parameters of this model are the vectors of the outside words, vectors of the center words
    - recall: the model takes the dot product of the outside vector (U) and dots it with the center word to output a 1-D vector. this vector is then passed through a softmax
- **optimization (gradient descent)**
    - to learn good word vectors, we have a loss function that we want to minimize
    - gradient descent is an algorithm to minimize the loss function by changing the parameters
        - the concept here is to compute the gradient from the current value of the parameters and then take a small step in the direction of the negative gradient
            - we do this because we want to minimize the loss
        - the formula looks like this: $\theta^{new} = \theta^{old} - \alpha * \nabla_{\theta}L(\theta)$
- **stochastic gradient descent**
    - the idea behind doing SGD is because it’s very expensive computing and fine tuning the parameters every single time
        - instead what we do is that we repeatedly sample windows and update after each one / small batch
            - in other words, instead of doing the entire dataset, we do smaller batches and update the dataset
- **word2vec algorithm family**
    - there are 2 model variants
        - skip-grams
            - they predict context words given the center word
        - continuous bag of words (CBOW)
            - predict the center word from a bag of context words