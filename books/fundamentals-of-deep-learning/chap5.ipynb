{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(input, weight_shape, bias_shape):\n",
    "    # Calculate the number of input neurons in the weight tensor\n",
    "    i_n = weight_shape[0] * weight_shape[1] * weight_shape[2]\n",
    "    \n",
    "    # Initialize the weights using random values from a normal distribution\n",
    "    weight_init = tf.random_normal_initializer(stddev=(2.0 / i_n) ** 0.5)\n",
    "    \n",
    "    # Create a TensorFlow variable for the weight tensor with the specified shape\n",
    "    # and initialize it using the weight_init initializer\n",
    "    W = tf.get_variable(\"W\", weight_shape, initializer=weight_init)\n",
    "    \n",
    "    # Initialize the bias term with a constant value of zero\n",
    "    bias_init = tf.constant_initializer(value=0)\n",
    "    \n",
    "    # Create a TensorFlow variable for the bias term with the specified shape\n",
    "    # and initialize it using the bias_init initializer\n",
    "    b = tf.get_variable(\"b\", bias_shape, initializer=bias_init)\n",
    "    \n",
    "    # Perform a 2D convolution on the input tensor using the weight tensor (W)\n",
    "    # with a stride of [1, 1, 1, 1] (one step at a time in all directions)\n",
    "    # and with padding='SAME' to ensure the output has the same dimensions as the input\n",
    "    conv_out = tf.nn.conv2d(input, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "    # Add the bias term (b) to the output of the convolutional operation (conv_out)\n",
    "    # and apply the ReLU activation function to introduce non-linearity\n",
    "    return tf.nn.relu(tf.nn.bias_add(conv_out, b))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool(input, k=2):\n",
    "    # Apply max-pooling operation on the input tensor 'input'.\n",
    "    # Max-pooling reduces the spatial dimensions of the input tensor\n",
    "    # by selecting the maximum value from each pooling region.\n",
    "    \n",
    "    # 'k' is the size of the max-pooling window (pool size) in both height and width.\n",
    "    # By default, it's set to 2x2, but you can change it by providing a different value.\n",
    "    # For example, k=2 means a 2x2 max-pooling window.\n",
    "\n",
    "    # tf.nn.max_pool performs the max-pooling operation with the specified parameters.\n",
    "    # The ksize argument defines the size of the pooling window for each dimension.\n",
    "    # It's set to [1, k, k, 1], where the first and last elements are always 1 (standard values)\n",
    "    # and the middle elements specify the pooling window size in height and width.\n",
    "\n",
    "    # The strides argument determines the step size of the pooling window in each dimension.\n",
    "    # It's set to [1, k, k, 1], meaning that the pooling window moves 'k' units at a time\n",
    "    # in both height and width directions.\n",
    "\n",
    "    # The padding argument is set to 'SAME', which means the output feature map will have\n",
    "    # the same spatial dimensions as the input by adding zero-padding at the edges if needed.\n",
    "\n",
    "    return tf.nn.max_pool(input, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(input, weight_shape, bias_shape):\n",
    "    # Calculate the standard deviation for weight initialization using the given weight_shape.\n",
    "    # The standard deviation is computed as (2.0 / number of input neurons) ** 0.5.\n",
    "    weight_stddev = (2.0 / weight_shape[0]) ** 0.5\n",
    "    \n",
    "    # Initialize the weights using random values from a normal distribution with the calculated standard deviation.\n",
    "    w_init = tf.random_normal_initializer(stddev=weight_stddev)\n",
    "    \n",
    "    # Initialize the bias term with a constant value of zero.\n",
    "    bias_init = tf.constant_initializer(value=0)\n",
    "    \n",
    "    # Create a TensorFlow variable for the weight tensor with the specified shape,\n",
    "    # and initialize it using the w_init initializer.\n",
    "    W = tf.get_variable(\"W\", weight_shape, initializer=w_init)\n",
    "    \n",
    "    # Create a TensorFlow variable for the bias term with the specified shape,\n",
    "    # and initialize it using the bias_init initializer.\n",
    "    b = tf.get_variable(\"b\", bias_shape, initializer=bias_init)\n",
    "    \n",
    "    # Perform a fully connected layer operation (matrix multiplication) on the input tensor using the weight tensor (W),\n",
    "    # add the bias term (b), and apply the ReLU activation function to introduce non-linearity.\n",
    "    return tf.nn.relu(tf.matmul(input, W) + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(x, keep_prob):\n",
    "    # Reshape the input tensor 'x' to have shape [-1, 28, 28, 1].\n",
    "    # '-1' in the first dimension allows the function to handle variable batch sizes.\n",
    "    # The input tensor represents grayscale images of size 28x28 pixels.\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "    \n",
    "    # First Convolutional Layer\n",
    "    with tf.variable_scope(\"conv_1\"):\n",
    "        # Perform a 2D convolution on the input tensor 'x' using 32 filters of size 5x5.\n",
    "        # The input has a single channel (grayscale) and the filters produce 32 output channels.\n",
    "        conv_1 = conv2d(x, [5, 5, 1, 32], [32])\n",
    "        \n",
    "        # Apply max-pooling to the output of the first convolutional layer ('conv_1').\n",
    "        # Reduce the spatial dimensions by selecting the maximum value from each 2x2 region.\n",
    "        pool_1 = max_pool(conv_1)\n",
    "    \n",
    "    # Second Convolutional Layer\n",
    "    with tf.variable_scope(\"conv_2\"):\n",
    "        # Perform a 2D convolution on the output of the first pooling layer ('pool_1').\n",
    "        # Use 64 filters of size 5x5 to process the 32-channel input and produce 64 output channels.\n",
    "        conv_2 = conv2d(pool_1, [5, 5, 32, 64], [64])\n",
    "        \n",
    "        # Apply max-pooling to the output of the second convolutional layer ('conv_2').\n",
    "        # Reduce the spatial dimensions further using 2x2 max-pooling.\n",
    "        pool_2 = max_pool(conv_2)\n",
    "    \n",
    "    # Fully Connected Layer\n",
    "    with tf.variable_scope(\"fc\"):\n",
    "        # Flatten the output of the second pooling layer ('pool_2') into a 1D tensor.\n",
    "        # The flattened tensor has shape [-1, 7 * 7 * 64], where '-1' represents the batch size.\n",
    "        pool_2_flat = tf.reshape(pool_2, [-1, 7 * 7 * 64])\n",
    "        \n",
    "        # Perform a fully connected layer operation on the flattened tensor ('pool_2_flat').\n",
    "        # Use 1024 neurons in this fully connected layer.\n",
    "        fc_1 = layer(pool_2_flat, [7 * 7 * 64, 1024], [1024])\n",
    "        \n",
    "        # Apply dropout to the output of the fully connected layer ('fc_1') with a probability of 'keep_prob'.\n",
    "        # Dropout helps prevent overfitting during training by randomly dropping some neurons during each step.\n",
    "        fc_1_drop = tf.nn.dropout(fc_1, keep_prob)\n",
    "    \n",
    "    # Output Layer\n",
    "    with tf.variable_scope(\"output\"):\n",
    "        # Perform another fully connected layer operation on the dropout output ('fc_1_drop').\n",
    "        # Use 10 neurons in this fully connected layer to produce the final output of the network.\n",
    "        output = layer(fc_1_drop, [1024, 10], [10])\n",
    "    \n",
    "    # Return the output tensor, which represents the predictions of the neural network.\n",
    "    return output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
